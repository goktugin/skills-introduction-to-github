{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO4OAL/ol/PmDxJUpa0FxTf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/goktugin/skills-introduction-to-github/blob/main/vcf_filtered_csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTGLuCnAfhbV",
        "outputId": "71acfa61-ef58-48c7-c9d1-7c2ba33a3772"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".vcf.gz dosyası indiriliyor: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "--2025-05-20 11:40:27--  http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.167\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.167|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 205612353 (196M) [application/x-gzip]\n",
            "Saving to: ‘ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz’\n",
            "\n",
            "ALL.chr22.phase3_sh 100%[===================>] 196.09M  24.5MB/s    in 8.2s    \n",
            "\n",
            "2025-05-20 11:40:36 (23.8 MB/s) - ‘ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz’ saved [205612353/205612353]\n",
            "\n",
            "\n",
            ".tbi dosyası indiriliyor: ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "--2025-05-20 11:40:36--  http://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.167\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.167|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36251 (35K) [application/x-gzip]\n",
            "Saving to: ‘ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi’\n",
            "\n",
            "ALL.chr22.phase3_sh 100%[===================>]  35.40K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-05-20 11:40:37 (360 KB/s) - ‘ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi’ saved [36251/36251]\n",
            "\n",
            "\n",
            "İndirme işlemleri tamamlandı. Dosyalar kontrol ediliyor:\n",
            "-rw-r--r-- 1 root root 197M Mar 16  2021 ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "-rw-r--r-- 1 root root  36K Mar 16  2021 ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n"
          ]
        }
      ],
      "source": [
        "# .vcf.gz dosyasının tam URL'sini buraya yazın\n",
        "vcf_gz_url = \"ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
        "\n",
        "# .vcf.gz.tbi dosyasının URL'si\n",
        "tbi_url = vcf_gz_url + \".tbi\"\n",
        "\n",
        "# Dosya adlarını URL'lerden çıkaralım (daha temiz bir kod için)\n",
        "vcf_gz_filename = vcf_gz_url.split(\"/\")[-1]\n",
        "tbi_filename = tbi_url.split(\"/\")[-1]\n",
        "\n",
        "print(f\".vcf.gz dosyası indiriliyor: {vcf_gz_filename}\")\n",
        "!wget {vcf_gz_url}\n",
        "\n",
        "print(f\"\\n.tbi dosyası indiriliyor: {tbi_filename}\")\n",
        "!wget {tbi_url}\n",
        "\n",
        "print(\"\\nİndirme işlemleri tamamlandı. Dosyalar kontrol ediliyor:\")\n",
        "!ls -lh {vcf_gz_filename} {tbi_filename}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from scipy.stats import f_oneway\n",
        "import numpy as np\n",
        "\n",
        "!pip install cyvcf2\n",
        "from cyvcf2 import VCF\n",
        "\n",
        "# İndirilecek VCF dosyası\n",
        "vcf_file_path = \"ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\"\n",
        "# Panel dosyasının URL'si ve Colab'e indirilecek adı\n",
        "panel_url = \"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\"\n",
        "panel_file_path = \"integrated_call_samples_v3.20130502.ALL.panel\" # wget ile indirilecek dosya adı\n",
        "\n",
        "print(f\"Panel dosyası indiriliyor: {panel_file_path}\")\n",
        "!wget -O {panel_file_path} {panel_url} # -O ile dosya adını belirtiyoruz\n",
        "print(\"\\nİndirme tamamlandı.\")\n",
        "!ls -lh {panel_file_path} # Dosyanın indiğini ve boyutunu kontrol et"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDGPkM_Ofn-7",
        "outputId": "98ee0f31-a32f-4619-d0fe-22e0b1392931"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cyvcf2\n",
            "  Downloading cyvcf2-0.31.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from cyvcf2) (2.0.2)\n",
            "Collecting coloredlogs (from cyvcf2)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from cyvcf2) (8.2.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->cyvcf2)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading cyvcf2-0.31.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, cyvcf2\n",
            "Successfully installed coloredlogs-15.0.1 cyvcf2-0.31.1 humanfriendly-10.0\n",
            "Panel dosyası indiriliyor: integrated_call_samples_v3.20130502.ALL.panel\n",
            "--2025-05-20 11:41:05--  ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/integrated_call_samples_v3.20130502.ALL.panel\n",
            "           => ‘integrated_call_samples_v3.20130502.ALL.panel’\n",
            "Resolving ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)... 193.62.193.167\n",
            "Connecting to ftp.1000genomes.ebi.ac.uk (ftp.1000genomes.ebi.ac.uk)|193.62.193.167|:21... connected.\n",
            "Logging in as anonymous ... Logged in!\n",
            "==> SYST ... done.    ==> PWD ... done.\n",
            "==> TYPE I ... done.  ==> CWD (1) /vol1/ftp/release/20130502 ... done.\n",
            "==> SIZE integrated_call_samples_v3.20130502.ALL.panel ... 55156\n",
            "==> PASV ... done.    ==> RETR integrated_call_samples_v3.20130502.ALL.panel ... done.\n",
            "Length: 55156 (54K) (unauthoritative)\n",
            "\n",
            "integrated_call_sam 100%[===================>]  53.86K   276KB/s    in 0.2s    \n",
            "\n",
            "2025-05-20 11:41:07 (276 KB/s) - ‘integrated_call_samples_v3.20130502.ALL.panel’ saved [55156]\n",
            "\n",
            "\n",
            "İndirme tamamlandı.\n",
            "-rw-r--r-- 1 root root 54K May 20 11:41 integrated_call_samples_v3.20130502.ALL.panel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Panel dosyası genellikle tab ile ayrılmıştır ve ilk satır başlıktır. Sütun adları: 'sample', 'pop', 'super_pop', 'gender'\n",
        "    panel_df = pd.read_csv(panel_file_path, sep='\\t')\n",
        "\n",
        "    # VCF dosyasındaki örnek ID'lerini alalım (cyvcf2 kullanarak)\n",
        "    vcf_reader_for_samples = VCF(vcf_file_path) #Dosyayı okumaya hazır bir nesne (vcf_reader_for_samples) oluşturulmuş olur.\n",
        "    vcf_samples = vcf_reader_for_samples.samples #.samples özelliği, VCF dosyasındaki bireylerin/örneklerin isimlerinin bir listesini döndürür.\n",
        "    vcf_reader_for_samples.close() # Okuyucuyu kapatalım\n",
        "\n",
        "    # Paneldeki örnekleri VCF'dekilerle filtreme\n",
        "    panel_df_filtered = panel_df[panel_df['sample'].isin(vcf_samples)]\n",
        "\n",
        "    # 'sample' ID'lerini 'super_pop' kodlarına eşleyen bir sözlük oluşturalım. Yani {'Sample1': 'EUR', 'Sample2': 'AFR', ...} gibi bir sonuç\n",
        "    sample_to_pop = pd.Series(panel_df_filtered.super_pop.values, index=panel_df_filtered['sample']).to_dict()\n",
        "\n",
        "    if not sample_to_pop:\n",
        "        print(\"UYARI: 'sample_to_pop' sözlüğü boş. Panel dosyası içeriği veya VCF örnekleriyle eşleşme kontrol edilmeli.\")\n",
        "        print(f\"Panel dosyasında bulunan örnek sayısı: {len(panel_df)}\")\n",
        "        print(f\"VCF dosyasında bulunan örnek sayısı: {len(vcf_samples)}\")\n",
        "        print(f\"Panelde VCF ile eşleşen örnek sayısı: {len(panel_df_filtered)}\")\n",
        "\n",
        "    print(f\"Panel dosyasından {len(sample_to_pop)} örnek için popülasyon bilgisi başarıyla yüklendi.\")\n",
        "    print(\"Örnek bir eşleşme (ilk 5):\", dict(list(sample_to_pop.items())[:5]))\n",
        "    print(\"Paneldeki benzersiz süper popülasyonlar:\", panel_df_filtered['super_pop'].unique())\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"HATA: Panel dosyası bulunamadı: {panel_file_path}\")\n",
        "    print(\"Lütfen panel dosyasının doğru indirildiğinden ve 'panel_file_path' değişkeninin doğru olduğundan emin olun.\")\n",
        "    sample_to_pop = {}\n",
        "except KeyError as e:\n",
        "    print(f\"HATA: Panel dosyasında beklenen sütun bulunamadı: {e}\")\n",
        "    print(\"Panel dosyasının sütun adlarını ('sample', 'super_pop' vb.) ve formatını kontrol edin.\")\n",
        "    sample_to_pop = {}\n",
        "except Exception as e:\n",
        "    print(f\"Panel dosyası işlenirken bir hata oluştu: {e}\")\n",
        "    sample_to_pop = {}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0L-0Ha1sfy-y",
        "outputId": "93ec2444-4a7f-4484-8e09-1d6d748264da"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Panel dosyasından 2504 örnek için popülasyon bilgisi başarıyla yüklendi.\n",
            "Örnek bir eşleşme (ilk 5): {'HG00096': 'EUR', 'HG00097': 'EUR', 'HG00099': 'EUR', 'HG00100': 'EUR', 'HG00101': 'EUR'}\n",
            "Paneldeki benzersiz süper popülasyonlar: ['EUR' 'EAS' 'AMR' 'SAS' 'AFR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y bcftools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5D78H3nuf3EM",
        "outputId": "8e183a52-dc0d-4929-a40a-8cae24e54489"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.83)] [1 InRelease 11.3 kB/129\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,683 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,387 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,944 kB]\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,726 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,948 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,540 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [77.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,547 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,255 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.2 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Fetched 31.9 MB in 4s (8,825 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libhts3 libhtscodecs2\n",
            "Suggested packages:\n",
            "  python3-numpy python3-matplotlib texlive-latex-recommended\n",
            "The following NEW packages will be installed:\n",
            "  bcftools libhts3 libhtscodecs2\n",
            "0 upgraded, 3 newly installed, 0 to remove and 89 not upgraded.\n",
            "Need to get 1,140 kB of archives.\n",
            "After this operation, 3,448 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhtscodecs2 amd64 1.1.1-3 [53.2 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhts3 amd64 1.13+ds-2build1 [390 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 bcftools amd64 1.13-1 [697 kB]\n",
            "Fetched 1,140 kB in 1s (1,028 kB/s)\n",
            "Selecting previously unselected package libhtscodecs2:amd64.\n",
            "(Reading database ... 126102 files and directories currently installed.)\n",
            "Preparing to unpack .../libhtscodecs2_1.1.1-3_amd64.deb ...\n",
            "Unpacking libhtscodecs2:amd64 (1.1.1-3) ...\n",
            "Selecting previously unselected package libhts3:amd64.\n",
            "Preparing to unpack .../libhts3_1.13+ds-2build1_amd64.deb ...\n",
            "Unpacking libhts3:amd64 (1.13+ds-2build1) ...\n",
            "Selecting previously unselected package bcftools.\n",
            "Preparing to unpack .../bcftools_1.13-1_amd64.deb ...\n",
            "Unpacking bcftools (1.13-1) ...\n",
            "Setting up libhtscodecs2:amd64 (1.1.1-3) ...\n",
            "Setting up libhts3:amd64 (1.13+ds-2build1) ...\n",
            "Setting up bcftools (1.13-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MAF (Minör Alel Frekansı) Filtrelemesi --> yaygın olan varyantları filtreliyoruz\n",
        "\n",
        "filtered_vcf_path = \"ALL.chr22.phase3_filtered_maf005.vcf.gz\" # Yeni filtrelenmiş dosya adı\n",
        "\n",
        "# ÖNEMLİ: bcftools'un MAF hesaplaması için INFO tag'ında AF (Allele Frequency) olması gerekir.\n",
        "# 1000 Genomes VCF'lerinde bu genellikle vardı veya INFO/AC (Allele Count) ve INFO/AN (Allele Number) üzerinden hesaplanabilir.\n",
        "# Eğer doğrudan MAF yoksa, 'AF > 0.05 && AF < 0.95' gibi bir filtre de kullanılabilir.\n",
        "# En basit haliyle AF (Alternatif Alel Frekansı) üzerinden gidelim:\n",
        "print(f\"MAF filtrelemesi başlıyor (AF > 0.05 ve AF < 0.95)...\")\n",
        "!bcftools view -i 'INFO/AF > 0.05 && INFO/AF < 0.95' {vcf_file_path} -Oz -o {filtered_vcf_path} # i --> include, INFO/AF --> INFO sütununda bulunan \"AF\"\n",
        "# minör alel frekansı (MAF) en az 0.05 olan varyantları tutar.\n",
        "# Yani hem çok nadir varyantları (MAF &lt; 0.05) hem de neredeyse popülasyonda sabitlenmiş\n",
        "# (alternatif alelin frekansı > 0.95, dolayısıyla referans alelin frekansı &lt; 0.05, yani MAF &lt; 0.05) varyantları çıkarır.\n",
        "# Kısacası, popülasyonda belirli bir düzeyde polimorfik (çeşitlilik gösteren) olan varyantları seçer.\n",
        "print(f\"Filtrelenmiş VCF dosyası oluşturuldu: {filtered_vcf_path}\")\n",
        "\n",
        "# Filtrelenmiş VCF için indeks oluşturmayı unutmayın (verimlilik artışı için)\n",
        "print(f\"Filtrelenmiş VCF için indeks oluşturuluyor...\")\n",
        "!bcftools index {filtered_vcf_path}\n",
        "print(f\"İndeks oluşturuldu: {filtered_vcf_path}.tbi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIims9f0f5EK",
        "outputId": "8816af71-c983-46ef-eba0-33af54e3555d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAF filtrelemesi başlıyor (AF > 0.05 ve AF < 0.95)...\n",
            "Filtrelenmiş VCF dosyası oluşturuldu: ALL.chr22.phase3_filtered_maf005.vcf.gz\n",
            "Filtrelenmiş VCF için indeks oluşturuluyor...\n",
            "İndeks oluşturuldu: ALL.chr22.phase3_filtered_maf005.vcf.gz.tbi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ----- GİRİŞ VE ÇIKIŞ DOSYA ADLARI -----\n",
        "# MAF ile filtrelediğiniz VCF dosyasının adı (bir önceki adımdaki çıktı)\n",
        "maf_filtered_vcf = \"ALL.chr22.phase3_filtered_maf005.vcf.gz\"\n",
        "\n",
        "# PLINK işlemleri için kullanılacak geçici dosya ön ekleri\n",
        "plink_input_prefix = \"chr22_maf_filtered_for_plink\"\n",
        "plink_ld_pruning_prefix = \"chr22_ld_pruned_list\"\n",
        "plink_final_pruned_prefix = \"chr22_maf_ld_pruned\"\n",
        "\n",
        "# LD Pruning sonrası ANOVA için kullanılacak nihai VCF dosyasının adı\n",
        "ld_pruned_vcf_for_anova = \"ALL.chr22.filtered_maf005_ldpruned.vcf.gz\"\n",
        "# ----- ----- ----- ----- ----- ----- -----\n",
        "\n",
        "# 1. GÜNCELLENMİŞ PLINK Kurulumu\n",
        "print(\"PLINK indiriliyor ve ayarlanıyor...\")\n",
        "# Önceki olası kalıntıları temizleyelim (daha güvenli bir kurulum için)\n",
        "!rm -f plink plink.zip plink.* 큰*.* *.log # plink, plink.zip ve PLINK ile gelen diğer dosyaları sil\n",
        "\n",
        "# GÜNCELLENMİŞ İNDİRME LİNKİ:\n",
        "plink_download_url = \"https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20231211.zip\"\n",
        "!wget {plink_download_url} -O plink.zip\n",
        "\n",
        "# wget'in başarılı olup olmadığını kontrol edelim (dosya boyutu > 0 olmalı)\n",
        "if os.path.exists(\"plink.zip\") and os.path.getsize(\"plink.zip\") > 0:\n",
        "    print(\"PLINK zip dosyası başarıyla indirildi.\")\n",
        "    !unzip -o plink.zip # -o ile üzerine yazma komutu (overwrite)\n",
        "    print(\"\\nZip'ten çıkarılan dosyalar:\")\n",
        "    !ls -l # Zip'ten çıkan dosyaları listele, 'plink' adında bir dosya görmeliyiz\n",
        "\n",
        "    if os.path.isfile(\"plink\"):\n",
        "        !chmod +x plink\n",
        "        print(\"PLINK başarıyla ayıklandı ve çalıştırılabilir yapıldı.\")\n",
        "    else:\n",
        "        print(\"HATA: PLINK çalıştırılabilir dosyası ('plink') zip'ten çıkarıldıktan sonra bulunamadı!\")\n",
        "        print(\"Lütfen yukarıdaki 'ls -l' çıktısını kontrol edin. Eğer farklı bir isimle çıkarıldıysa, kodda './plink' yerine o ismi kullanmanız gerekir.\")\n",
        "        raise FileNotFoundError(\"PLINK executable ('plink') not found after unzip.\")\n",
        "else:\n",
        "    print(f\"HATA: PLINK zip dosyası ({plink_download_url}) indirilemedi veya boş. Lütfen linki kontrol edin veya çalışan başka bir PLINK 1.9 indirme linki bulun.\")\n",
        "    raise FileNotFoundError(\"PLINK zip file download failed or the file is empty.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZY_IpjPgEUh",
        "outputId": "fd6e754c-dfe1-4a44-c102-509857ddcd2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PLINK indiriliyor ve ayarlanıyor...\n",
            "--2025-05-20 11:45:09--  https://s3.amazonaws.com/plink1-assets/plink_linux_x86_64_20231211.zip\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.214.128, 52.216.238.13, 3.5.31.130, ...\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.214.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8953953 (8.5M) [application/zip]\n",
            "Saving to: ‘plink.zip’\n",
            "\n",
            "plink.zip           100%[===================>]   8.54M  32.0MB/s    in 0.3s    \n",
            "\n",
            "2025-05-20 11:45:09 (32.0 MB/s) - ‘plink.zip’ saved [8953953/8953953]\n",
            "\n",
            "PLINK zip dosyası başarıyla indirildi.\n",
            "Archive:  plink.zip\n",
            "  inflating: plink                   \n",
            "  inflating: LICENSE                 \n",
            "  inflating: toy.ped                 \n",
            "  inflating: toy.map                 \n",
            "  inflating: prettify                \n",
            "\n",
            "Zip'ten çıkarılan dosyalar:\n",
            "total 349972\n",
            "-rw-r--r-- 1 root root 101873401 May 20 11:44 ALL.chr22.phase3_filtered_maf005.vcf.gz\n",
            "-rw-r--r-- 1 root root     23421 May 20 11:44 ALL.chr22.phase3_filtered_maf005.vcf.gz.csi\n",
            "-rw-r--r-- 1 root root 205612353 Mar 16  2021 ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz\n",
            "-rw-r--r-- 1 root root     36251 Mar 16  2021 ALL.chr22.phase3_shapeit2_mvncall_integrated_v5b.20130502.genotypes.vcf.gz.tbi\n",
            "-rw-r--r-- 1 root root     55156 May 20 11:41 integrated_call_samples_v3.20130502.ALL.panel\n",
            "-rw-rw-r-- 1 root root     35147 Dec 12  2023 LICENSE\n",
            "-rwxrwxr-x 1 root root  41730792 Dec 12  2023 plink\n",
            "-rw-r--r-- 1 root root   8953953 Dec 12  2023 plink.zip\n",
            "-rwxrwxr-x 1 root root     18336 Mar  6  2019 prettify\n",
            "drwxr-xr-x 1 root root      4096 May 14 13:38 sample_data\n",
            "-rw-rw-r-- 1 root root        27 Dec 12  2023 toy.map\n",
            "-rw-rw-r-- 1 root root        58 Dec 12  2023 toy.ped\n",
            "PLINK başarıyla ayıklandı ve çalıştırılabilir yapıldı.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import os\n",
        "import re # PLINK çıktısından bazı bilgileri çekmek için\n",
        "\n",
        "# ----- GİRİŞ VE ÇIKIŞ DOSYA ADLARI (bir öncekiyle aynı) -----\n",
        "maf_filtered_vcf = \"ALL.chr22.phase3_filtered_maf005.vcf.gz\"\n",
        "vcf_with_ids = \"ALL.chr22.phase3_filtered_maf005.ids.vcf.gz\"\n",
        "plink_input_prefix = \"chr22_maf_ids_for_plink\"\n",
        "plink_ld_pruning_prefix = \"chr22_ld_pruned_list\"\n",
        "plink_final_pruned_prefix = \"chr22_maf_ids_ld_pruned\"\n",
        "ld_pruned_vcf_for_anova = \"ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\"\n",
        "plink_executable = \"./plink\"\n",
        "# ----- ----- ----- ----- ----- ----- ----- ----- -----\n",
        "\n",
        "def run_command(cmd_list, log_prefix_for_error=\"cmd\"):\n",
        "    \"\"\"Yardımcı fonksiyon: Komut çalıştırır, başarı/hata durumunu yönetir.\"\"\"\n",
        "    print(f\"Komut çalıştırılıyor: {' '.join(cmd_list[:4])} ...\") # Komutun başını göster\n",
        "    log_file = f\"{log_prefix_for_error}.log\"\n",
        "    # subprocess.run'dan önce log dosyasını silmeyin, PLINK kendisi oluşturur/üzerine yazar\n",
        "\n",
        "    result = subprocess.run(cmd_list, capture_output=True, text=True, check=False)\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        print(f\"BAŞARILI: Komut tamamlandı. (Log: {log_file if os.path.exists(log_file) else 'oluşturulmadı'})\")\n",
        "        # Başarılıysa sadece kısa bir STDERR özeti (uyarılar için)\n",
        "        if result.stderr.strip():\n",
        "            print(\"  PLINK STDERR (Uyarılar olabilir):\")\n",
        "            # Sadece ilk birkaç satırı veya önemli uyarıları göster\n",
        "            stderr_lines = result.stderr.strip().split('\\n')\n",
        "            for line in stderr_lines[:5]: # İlk 5 satır\n",
        "                print(f\"    {line}\")\n",
        "            if len(stderr_lines) > 5:\n",
        "                print(\"    ...\")\n",
        "        return result.stdout # Başarılı durumda STDOUT'u döndür (işlemek için)\n",
        "    else:\n",
        "        print(f\"HATA: Komut başarısız oldu. Dönüş kodu: {result.returncode}\")\n",
        "        print(\"  --- PLINK STDOUT ---\")\n",
        "        print(result.stdout)\n",
        "        print(\"  --- PLINK STDERR ---\")\n",
        "        print(result.stderr)\n",
        "        if os.path.exists(log_file):\n",
        "            print(f\"\\n  --- {log_file} içeriği ---\")\n",
        "            with open(log_file, 'r') as f:\n",
        "                print(f.read())\n",
        "            print(f\"  --- {log_file} sonu ---\")\n",
        "        raise RuntimeError(f\"Komut başarısız: {' '.join(cmd_list[:3])}...\")\n",
        "\n",
        "# --- Adım 0: Dosya Kontrolleri (kısaltılmış) ---\n",
        "if not (os.path.isfile(plink_executable) and os.access(plink_executable, os.X_OK)):\n",
        "    raise FileNotFoundError(f\"PLINK executable not found or not executable: {plink_executable}\")\n",
        "if not os.path.exists(maf_filtered_vcf):\n",
        "    raise FileNotFoundError(f\"Input VCF file not found: {maf_filtered_vcf}\")\n",
        "print(\"PLINK ve girdi VCF dosyası bulundu.\")\n",
        "\n",
        "# --- Adım 1: bcftools annotate\n",
        "print(f\"\\nAdım 1: '{maf_filtered_vcf}' dosyasındaki eksik ID'ler bcftools annotate ile dolduruluyor...\")\n",
        "bcftools_annotate_cmd = [\"bcftools\", \"annotate\", \"--set-id\", \"+'%CHROM:%POS:%REF:%FIRST_ALT'\", maf_filtered_vcf, \"-Oz\", \"-o\", vcf_with_ids]\n",
        "run_command(bcftools_annotate_cmd, \"bcftools_annotate\")\n",
        "!bcftools index {vcf_with_ids}\n",
        "print(f\"{vcf_with_ids} dosyası oluşturuldu ve indekslendi.\")\n",
        "\n",
        "# --- Adım 2: VCF to BED (PLINK) ---\n",
        "print(f\"\\nAdım 2: '{vcf_with_ids}' dosyası PLINK BED formatına dönüştürülüyor...\")\n",
        "plink_cmd_step2 = [plink_executable, \"--vcf\", vcf_with_ids, \"--allow-extra-chr\", \"--make-bed\", \"--out\", plink_input_prefix]\n",
        "stdout_step2 = run_command(plink_cmd_step2, plink_input_prefix)\n",
        "# PLINK'in yüklediği varyant sayısını STDOUT'tan veya log'dan alabiliriz (isteğe bağlı)\n",
        "variants_loaded_match = re.search(r\"(\\d+) variants loaded from .bim file\", stdout_step2)\n",
        "if variants_loaded_match:\n",
        "    print(f\"  {variants_loaded_match.group(1)} varyant PLINK BED formatına yüklendi.\")\n",
        "print(f\"PLINK BED formatına dönüştürme başarılı.\")\n",
        "\n",
        "# LD Pruning, yüksek LD'de olan SNP gruplarından sadece bir veya birkaç temsilci SNP'yi tutarak diğerlerini veri setinden çıkarır. Bu sayede, PCA için daha \"bağımsız\" ve daha az sayıda SNP içeren bir set elde edilir, bu da daha güvenilir ve yorumlanabilir popülasyon yapısı sonuçları verir.\n",
        "# --- Adım 3: LD Pruning (PLINK) ---\n",
        "window_size_snps = 50; step_size_snps = 5; r2_threshold = 0.2\n",
        "print(f\"\\nAdım 3: LD Pruning işlemi başlıyor...\")\n",
        "plink_cmd_step3 = [plink_executable, \"--bfile\", plink_input_prefix, \"--allow-extra-chr\", \"--indep-pairwise\", str(window_size_snps), str(step_size_snps), str(r2_threshold), \"--out\", plink_ld_pruning_prefix]\n",
        "stdout_step3 = run_command(plink_cmd_step3, plink_ld_pruning_prefix)\n",
        "pruned_match = re.search(r\"Pruning complete.  (\\d+) of \\d+ variants removed\", stdout_step3)\n",
        "if pruned_match:\n",
        "    print(f\"  LD Pruning tamamlandı. {pruned_match.group(1)} varyant çıkarıldı.\")\n",
        "else: # Logdan okumayı deneyebiliriz veya sadece genel mesaj\n",
        "    print(f\"  LD Pruning tamamlandı. Ayrıntılar için {plink_ld_pruning_prefix}.log dosyasına bakınız.\")\n",
        "print(f\"LD Pruning başarılı.\")\n",
        "\n",
        "# --- Adım 4: Budanmış Seti Oluşturma (PLINK) ---\n",
        "print(f\"\\nAdım 4: Budanmış SNP listesi kullanılarak yeni PLINK dosyası oluşturuluyor...\")\n",
        "plink_cmd_step4 = [plink_executable, \"--bfile\", plink_input_prefix, \"--allow-extra-chr\", \"--extract\", f\"{plink_ld_pruning_prefix}.prune.in\", \"--make-bed\", \"--out\", plink_final_pruned_prefix]\n",
        "stdout_step4 = run_command(plink_cmd_step4, plink_final_pruned_prefix)\n",
        "variants_remaining_match = re.search(r\"--extract: (\\d+) variants remaining\", stdout_step4)\n",
        "if variants_remaining_match:\n",
        "    print(f\"  {variants_remaining_match.group(1)} SNP ile yeni PLINK seti oluşturuldu.\")\n",
        "print(f\"Yeni budanmış PLINK dosyası başarıyla oluşturuldu.\")\n",
        "\n",
        "# --- Adım 5: VCF'ye Geri Dönüşüm (PLINK) ---\n",
        "output_vcf_prefix_for_plink = ld_pruned_vcf_for_anova.replace(\".vcf.gz\", \"\")\n",
        "print(f\"\\nAdım 5: Son budanmış PLINK seti VCF formatına dönüştürülüyor: {ld_pruned_vcf_for_anova}\")\n",
        "plink_cmd_step5 = [plink_executable, \"--bfile\", plink_final_pruned_prefix, \"--allow-extra-chr\", \"--recode\", \"vcf-iid\", \"bgz\", \"--out\", output_vcf_prefix_for_plink]\n",
        "run_command(plink_cmd_step5, output_vcf_prefix_for_plink)\n",
        "if os.path.exists(f\"{output_vcf_prefix_for_plink}.vcf.gz\"):\n",
        "    if f\"{output_vcf_prefix_for_plink}.vcf.gz\" != ld_pruned_vcf_for_anova:\n",
        "         os.rename(f\"{output_vcf_prefix_for_plink}.vcf.gz\", ld_pruned_vcf_for_anova)\n",
        "print(f\"VCF formatına dönüştürme başarılı.\")\n",
        "\n",
        "# --- Adım 6: İndeksleme (bcftools) ---\n",
        "print(f\"\\nAdım 6: {ld_pruned_vcf_for_anova} dosyası bcftools ile indeksleniyor...\")\n",
        "!bcftools index {ld_pruned_vcf_for_anova}\n",
        "print(f\"İndeksleme başarılı.\")\n",
        "\n",
        "print(f\"\\nLD Pruning tamamlandı! ANOVA analizi için kullanılacak dosya: {ld_pruned_vcf_for_anova}\")\n",
        "!ls -lh {ld_pruned_vcf_for_anova}* # Son dosyayı ve indeksini göster"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9u_o2L2kgwlB",
        "outputId": "616e752c-f188-4e01-a6b6-6a1d48f9ac2b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PLINK ve girdi VCF dosyası bulundu.\n",
            "\n",
            "Adım 1: 'ALL.chr22.phase3_filtered_maf005.vcf.gz' dosyasındaki eksik ID'ler bcftools annotate ile dolduruluyor...\n",
            "Komut çalıştırılıyor: bcftools annotate --set-id +'%CHROM:%POS:%REF:%FIRST_ALT' ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: oluşturulmadı)\n",
            "ALL.chr22.phase3_filtered_maf005.ids.vcf.gz dosyası oluşturuldu ve indekslendi.\n",
            "\n",
            "Adım 2: 'ALL.chr22.phase3_filtered_maf005.ids.vcf.gz' dosyası PLINK BED formatına dönüştürülüyor...\n",
            "Komut çalıştırılıyor: ./plink --vcf ALL.chr22.phase3_filtered_maf005.ids.vcf.gz --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: chr22_maf_ids_for_plink.log)\n",
            "  112555 varyant PLINK BED formatına yüklendi.\n",
            "PLINK BED formatına dönüştürme başarılı.\n",
            "\n",
            "Adım 3: LD Pruning işlemi başlıyor...\n",
            "Komut çalıştırılıyor: ./plink --bfile chr22_maf_ids_for_plink --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: chr22_ld_pruned_list.log)\n",
            "  LD Pruning tamamlandı. 102214 varyant çıkarıldı.\n",
            "LD Pruning başarılı.\n",
            "\n",
            "Adım 4: Budanmış SNP listesi kullanılarak yeni PLINK dosyası oluşturuluyor...\n",
            "Komut çalıştırılıyor: ./plink --bfile chr22_maf_ids_for_plink --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: chr22_maf_ids_ld_pruned.log)\n",
            "  10341 SNP ile yeni PLINK seti oluşturuldu.\n",
            "Yeni budanmış PLINK dosyası başarıyla oluşturuldu.\n",
            "\n",
            "Adım 5: Son budanmış PLINK seti VCF formatına dönüştürülüyor: ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "Komut çalıştırılıyor: ./plink --bfile chr22_maf_ids_ld_pruned --allow-extra-chr ...\n",
            "BAŞARILI: Komut tamamlandı. (Log: ALL.chr22.filtered_maf005_ids_ldpruned.log)\n",
            "VCF formatına dönüştürme başarılı.\n",
            "\n",
            "Adım 6: ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz dosyası bcftools ile indeksleniyor...\n",
            "İndeksleme başarılı.\n",
            "\n",
            "LD Pruning tamamlandı! ANOVA analizi için kullanılacak dosya: ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "-rw-r--r-- 1 root root 6.6M May 20 11:46 ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "-rw-r--r-- 1 root root  19K May 20 11:46 ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz.csi\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time # Zaman takibi için\n",
        "\n",
        "# ----- ANOVA İÇİN GİRDİ DOSYASI -----\n",
        "# LD Pruning sonrası oluşan VCF dosyasının adı:\n",
        "vcf_file_path = \"ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\"\n",
        "# ----- ----- ----- ----- ----- -----\n",
        "\n",
        "# sample_to_pop sözlüğünün ve diğer gerekli değişkenlerin\n",
        "# (min_samples_per_pop_for_calc, top_n_snps_to_select)\n",
        "# bir önceki hücrelerden yüklendiğini varsayıyoruz.\n",
        "# Eğer tanımlı değillerse, burada tekrar tanımlamanız gerekir:\n",
        "\n",
        "if 'sample_to_pop' not in locals() or not sample_to_pop:\n",
        "    print(\"HATA: 'sample_to_pop' sözlüğü bulunamadı. Lütfen popülasyon bilgilerini yükleme adımını tekrar çalıştırın.\")\n",
        "    raise NameError(\"'sample_to_pop' sözlüğü tanımlı değil veya boş.\")\n",
        "\n",
        "if 'min_samples_per_pop_for_calc' not in locals():\n",
        "    min_samples_per_pop_for_calc = 5 # Popülasyon başına ANOVA için min örnek\n",
        "\n",
        "if 'top_n_snps_to_select' not in locals():\n",
        "    top_n_snps_to_select = 5000 # Seçilecek en iyi SNP sayısı\n",
        "\n",
        "# --- ANOVA Analiz Kodu ---\n",
        "try:\n",
        "    vcf_reader = VCF(vcf_file_path)\n",
        "except OSError as e:\n",
        "    print(f\"HATA: VCF dosyası ({vcf_file_path}) veya indeksi bulunamadı/okunamadı: {e}\")\n",
        "    raise\n",
        "\n",
        "samples_in_vcf = vcf_reader.samples\n",
        "pop_to_samples = defaultdict(list) #Amacımız, her bir süper popülasyon kodunu (örn: 'EUR', 'AFR') anahtar olarak ve o popülasyona ait örnek ID'lerinin bir listesini de değer olarak tutan bir sözlük oluşturmaktır. pop_to_samples bu sözlük olacaktır.\n",
        "for sample_id, super_pop_code in sample_to_pop.items():\n",
        "    if sample_id in samples_in_vcf:\n",
        "         pop_to_samples[super_pop_code].append(sample_id)\n",
        "\n",
        "sample_indices = {sample_name: i for i, sample_name in enumerate(samples_in_vcf)}\n",
        "all_calculated_snps_with_pvalues = [] #ANOVA analizi sırasında, her bir SNP için bir p-değeri hesaplayacağız. Eğer bu p-değeri geçerliyse (NaN değilse), o SNP'ye ait bilgileri (kromozom, pozisyon, ID, p-değeri ve ANOVA'ya dahil edilen popülasyonlar) bir demet (tuple) olarak bu listeye ekleyeceğiz. Döngü tamamlandığında, bu liste, p-değeri hesaplanabilmiş tüm SNP'leri içerecektir. Daha sonra bu listeyi p-değerine göre sıralayıp en iyi N tanesini seçeceğiz.\n",
        "processed_variants_count = 0 #VCF dosyasındaki varyantlar (SNP'ler) üzerinde dönerken, o ana kadar kaç tane varyantın işlendiğini (yani döngünün kaçıncı adımında olduğumuzu) saymak için kullanılacak bir sayaç\n",
        "total_variants_in_file = 0 # VCF dosyasındaki toplam varyant satırı sayısını (yorum satırları ve başlık hariç) saymak için kullanılır. Döngü her bir varyant için çalıştığında bu sayaç artırılır. Analiz bittikten sonra, VCF dosyasında toplam kaç varyant olduğunu ve bunlardan kaçının işlendiğini raporlamak için kullanılır.\n",
        "\n",
        "if pop_to_samples:\n",
        "    print(f\"ANOVA analizi '{vcf_file_path}' dosyası üzerinde başlıyor...\")\n",
        "    print(f\"Popülasyon başına ANOVA için minimum örnek sayısı: {min_samples_per_pop_for_calc}\")\n",
        "    print(f\"Analiz sonunda p-değeri en küçük ilk {top_n_snps_to_select} SNP seçilecektir.\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for variant in vcf_reader: # Bu döngü şimdi çok daha hızlıdır\n",
        "        total_variants_in_file += 1\n",
        "        processed_variants_count += 1\n",
        "\n",
        "        if processed_variants_count % 2000 == 0: # İlerleme bildirimi\n",
        "            print(f\"{processed_variants_count} varyant işlendi...\")\n",
        "\n",
        "        if not (len(variant.ALT) == 1 and len(variant.REF) == 1 and len(variant.ALT[0]) == 1):\n",
        "            continue\n",
        "\n",
        "        # Her yeni varyant için, ANOVA'ya girecek genotip değerlerini ve\n",
        "        # bu ANOVA'ya dahil edilecek popülasyonları saklamak üzere boş listeler oluşturulur.\n",
        "        pop_genotype_values_for_anova = []\n",
        "        populations_in_anova = []\n",
        "\n",
        "        # 'pop_to_samples' sözlüğü üzerinde döngüye girilir. Bu sözlük, her bir süper popülasyon kodunu ('pop_code')\n",
        "        # o popülasyona ait örnek ID'lerinin bir listesine ('pop_sample_list') eşler.\n",
        "        # Örn: pop_code = 'EUR', pop_sample_list = ['HG00096', 'HG00097', ...]\n",
        "        for pop_code, pop_sample_list in pop_to_samples.items():\n",
        "            # O anki popülasyona ('pop_code') ait olup, aynı zamanda VCF dosyamızda da bulunan\n",
        "            # (yani 'sample_indices' sözlüğünde anahtarı olan) örnek ID'lerini seçer.\n",
        "            current_pop_samples_in_vcf = [s_id for s_id in pop_sample_list if s_id in sample_indices]\n",
        "\n",
        "            # Eğer bu popülasyondaki (VCF'de de bulunan) örnek sayısı, belirlediğimiz minimum eşikten\n",
        "            # ('min_samples_per_pop_for_calc', örn: 5) az ise, bu popülasyon bu SNP için ANOVA'ya\n",
        "            # dahil edilmez ve döngünün bir sonraki popülasyonuna geçilir ('continue').\n",
        "            # Bu, çok az bireye sahip grupların istatistiksel sonuçları aşırı etkilemesini önler.\n",
        "            if len(current_pop_samples_in_vcf) < min_samples_per_pop_for_calc:\n",
        "                continue\n",
        "\n",
        "            # Bu popülasyondaki ('current_pop_samples_in_vcf') bireylerin, o anki varyant için\n",
        "            # sahip oldukları alternatif alel sayılarını toplayacağımız boş bir liste.\n",
        "            individual_alt_allele_counts_in_pop = []\n",
        "\n",
        "            # Popülasyondaki her bir örnek ('sample_name') için döngüye girilir.\n",
        "            for sample_name in current_pop_samples_in_vcf:\n",
        "                idx = sample_indices[sample_name]  # Örnek ID'sinin VCF'deki sayısal indeksini alır.\n",
        "                genotype = variant.genotypes[idx]  # O örneğin, o anki varyanttaki genotipini alır.\n",
        "                                                   # genotype bir listedir: [alel1_indeksi, alel2_indeksi, faz_bilgisi]\n",
        "                                                   # alel_indeksi: 0 (referans alel), 1 (ilk alternatif alel), -1 (eksik veri).\n",
        "\n",
        "                alt_count_for_sample = 0     # Bu birey için o anki SNP'deki alternatif alel sayısı.\n",
        "                valid_alleles_for_sample = 0 # Bu birey için geçerli (eksik olmayan) alel sayısı.\n",
        "\n",
        "                # Bireyin ilk alelini kontrol et:\n",
        "                if genotype[0] != -1:  # Eğer alel eksik veri (-1) değilse:\n",
        "                    alt_count_for_sample += genotype[0] # alel1_indeksi (0 veya 1) eklenir.\n",
        "                    valid_alleles_for_sample += 1       # Geçerli alel sayısını artır.\n",
        "\n",
        "                # Bireyin ikinci alelini kontrol et:\n",
        "                if genotype[1] != -1:  # Eğer alel eksik veri (-1) değilse:\n",
        "                    alt_count_for_sample += genotype[1] # alel2_indeksi (0 veya 1) eklenir.\n",
        "                    valid_alleles_for_sample += 1       # Geçerli alel sayısını artır.\n",
        "\n",
        "                # Eğer bireyin en az bir aleli geçerliyse (yani genotip tamamen ' ./. ' değilse),\n",
        "                # hesaplanan alternatif alel sayısını (0, 1 veya 2 olabilir) listeye ekle.\n",
        "                if valid_alleles_for_sample > 0 :\n",
        "                    individual_alt_allele_counts_in_pop.append(alt_count_for_sample)\n",
        "\n",
        "            # Eğer bu popülasyondan, minimum örnek sayısını ('min_samples_per_pop_for_calc')\n",
        "            # karşılayacak kadar bireyden genotip verisi (alternatif alel sayısı) toplayabildiysek:\n",
        "            if len(individual_alt_allele_counts_in_pop) >= min_samples_per_pop_for_calc:\n",
        "                # Bu popülasyonun genotip değerlerini (bir NumPy dizisi olarak) ANOVA'ya girecek\n",
        "                # genel listeye ('pop_genotype_values_for_anova') ekle.\n",
        "                pop_genotype_values_for_anova.append(np.array(individual_alt_allele_counts_in_pop))\n",
        "                # Bu popülasyonun kodunu ('pop_code') da ANOVA'ya dahil edilen popülasyonlar\n",
        "                # listesine ('populations_in_anova') ekle.\n",
        "                populations_in_anova.append(pop_code)\n",
        "\n",
        "        # Tüm popülasyonlar için veri toplama işlemi bittikten sonra,\n",
        "        # ANOVA testi yapabilmek için en az 2 popülasyon grubundan veri toplamış olmamız gerekir.\n",
        "        if len(pop_genotype_values_for_anova) >= 2:\n",
        "            try:\n",
        "                # scipy.stats.f_oneway fonksiyonu ile tek yönlü ANOVA testi yapılır.\n",
        "                # '*' operatörü, 'pop_genotype_values_for_anova' listesindeki her bir NumPy dizisini\n",
        "                # (yani her bir popülasyonun alternatif alel sayıları listesini) fonksiyona\n",
        "                # ayrı bir argüman olarak gönderir. Her bir argüman, ANOVA'da bir grup olarak kabul edilir.\n",
        "                # Fonksiyon, F istatistiğini ve p-değerini döndürür.\n",
        "                f_stat, p_value = f_oneway(*pop_genotype_values_for_anova)\n",
        "\n",
        "                # Eğer hesaplanan p-değeri geçerli bir sayı ise (NaN - Not a Number - değilse):\n",
        "                if not np.isnan(p_value):\n",
        "                    # O anki varyantın Kromozom, Pozisyon, ID'si (bcftools ile atanan),\n",
        "                    # hesaplanan p-değeri ve bu ANOVA'ya dahil edilen popülasyonların listesini\n",
        "                    # bir demet (tuple) olarak 'all_calculated_snps_with_pvalues' listesine ekle.\n",
        "                    all_calculated_snps_with_pvalues.append((variant.CHROM, variant.POS, variant.ID, p_value, populations_in_anova))\n",
        "            except Exception as e:\n",
        "                # ANOVA hesaplaması sırasında bir hata oluşursa (örn: tüm gruplardaki tüm değerler aynıysa\n",
        "                # ve varyans sıfırsa, f_oneway uyarı verebilir veya hata fırlatabilir),\n",
        "                # bu hatayı sessizce atla ('pass') ve bir sonraki varyanta geç.\n",
        "                # İstenirse, hata ayıklama için buraya print(e) eklenebilir.\n",
        "                pass\n",
        "\n",
        "    end_time = time.time()\n",
        "    total_elapsed_time = end_time - start_time\n",
        "    print(f\"\\nANOVA hesaplamaları tamamlandı.\")\n",
        "    print(f\"Toplam {processed_variants_count} varyant (dosyadaki toplam: {total_variants_in_file}) işlendi.\")\n",
        "    print(f\"Toplam geçen süre: {total_elapsed_time:.2f} saniye.\")\n",
        "    print(f\"{len(all_calculated_snps_with_pvalues)} SNP için geçerli p-değeri hesaplandı.\")\n",
        "\n",
        "    all_calculated_snps_with_pvalues.sort(key=lambda x: x[3]) # p-value (indeks 3) göre sırala\n",
        "\n",
        "    final_selected_snps = all_calculated_snps_with_pvalues[:top_n_snps_to_select]\n",
        "    print(f\"\\nP-değeri en küçük ilk {len(final_selected_snps)} SNP seçildi (hedeflenen: {top_n_snps_to_select}).\")\n",
        "\n",
        "    print(f\"\\nSeçilen ilk {min(20, len(final_selected_snps))} SNP (toplam {len(final_selected_snps)} adet seçildi):\")\n",
        "    for i, snp_info in enumerate(final_selected_snps[:20]): # (CHROM, POS, ID, p_value, populations_in_anova)\n",
        "        print(f\"SNP: {snp_info[0]}:{snp_info[1]} (ID: {snp_info[2]}), P-value: {snp_info[3]:.4e}, Popülasyonlar: {snp_info[4]}\")\n",
        "    if not final_selected_snps:\n",
        "        print(\"Hiç SNP seçilemedi.\")\n",
        "else:\n",
        "    print(\"Popülasyon bilgileri ('pop_to_samples') yüklenemediği için ANOVA analizi yapılamıyor.\")\n",
        "vcf_reader.close()\n",
        "\n",
        "# final_selected_snps listesi artık sonraki adımlar için kullanılabilir.\n",
        "# Örnek: Sadece SNP pozisyonlarını ve ID'lerini bir listeye alalım\n",
        "# selected_snp_details_for_matrix = [(item[0], item[1], item[2]) for item in final_selected_snps]\n",
        "# print(f\"\\nGenotip matrisi için seçilen {len(selected_snp_details_for_matrix)} SNP detayı: {selected_snp_details_for_matrix[:5]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0qE4ping0hG",
        "outputId": "b6f47259-80cf-4589-d7a2-0808bcac9be2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ANOVA analizi 'ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz' dosyası üzerinde başlıyor...\n",
            "Popülasyon başına ANOVA için minimum örnek sayısı: 5\n",
            "Analiz sonunda p-değeri en küçük ilk 5000 SNP seçilecektir.\n",
            "2000 varyant işlendi...\n",
            "4000 varyant işlendi...\n",
            "6000 varyant işlendi...\n",
            "8000 varyant işlendi...\n",
            "10000 varyant işlendi...\n",
            "\n",
            "ANOVA hesaplamaları tamamlandı.\n",
            "Toplam 10341 varyant (dosyadaki toplam: 10341) işlendi.\n",
            "Toplam geçen süre: 44.61 saniye.\n",
            "7703 SNP için geçerli p-değeri hesaplandı.\n",
            "\n",
            "P-değeri en küçük ilk 5000 SNP seçildi (hedeflenen: 5000).\n",
            "\n",
            "Seçilen ilk 20 SNP (toplam 5000 adet seçildi):\n",
            "SNP: 22:17858962 (ID: '22:17858962:T:C'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:18540577 (ID: '22:18540577:T:C'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:20499061 (ID: '22:20499061:T:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:20950079 (ID: '22:20950079:T:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:22568112 (ID: '22:22568112:T:C'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:23290519 (ID: '22:23290519:G:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:23448170 (ID: '22:23448170:G:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:25073602 (ID: '22:25073602:C:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:26422980 (ID: '22:26422980:A:G'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:26438495 (ID: '22:26438495:A:G'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:26505565 (ID: '22:26505565:A:G'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:27831461 (ID: '22:27831461:T:C'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:32621269 (ID: '22:32621269:C:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:32916292 (ID: '22:32916292:C:T'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:33665664 (ID: '22:33665664:A:G'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:33767245 (ID: '22:33767245:T:A'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:33785111 (ID: '22:33785111:A:G'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:34060798 (ID: '22:34060798:C:T'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:34284609 (ID: '22:34284609:C:T'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n",
            "SNP: 22:35306786 (ID: '22:35306786:C:T'), P-value: 0.0000e+00, Popülasyonlar: ['EUR', 'EAS', 'AMR', 'SAS', 'AFR']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- Gerekli Değişkenler (bir önceki adımlardan gelmeli) -----\n",
        "# ANOVA sonrası seçilen SNP listesi ( (CHROM, POS, ID, p_value, populations) formatında)\n",
        "# Bu listenin bir önceki ANOVA adımından geldiğini varsayıyoruz.\n",
        "# Eğer hücreyi yeniden başlattıysanız, final_selected_snps'i tekrar oluşturmanız gerekebilir.\n",
        "if 'final_selected_snps' not in locals() or not final_selected_snps:\n",
        "    print(\"HATA: 'final_selected_snps' listesi bulunamadı veya boş.\")\n",
        "    print(\"Lütfen ANOVA analizini çalıştırdığınızdan ve bu listenin oluştuğundan emin olun.\")\n",
        "    # Örnek olarak boş bir liste ile devam etmesini engelleyelim, kullanıcı fark etsin.\n",
        "    raise NameError(\"'final_selected_snps' tanımlı değil veya ANOVA sonucu boş.\")\n",
        "\n",
        "# Genotiplerin okunacağı VCF dosyası (LD pruning sonrası oluşan dosya)\n",
        "vcf_file_for_genotypes = \"ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\"\n",
        "\n",
        "# Çıktı csv dosyasının adı\n",
        "output_csv_file = \"genotip_matrisi_top5000_snps.csv\"\n",
        "# ----- ----- ----- ----- ----- ----- ----- ----- ----- -----\n",
        "\n",
        "print(f\"Seçilen {len(final_selected_snps)} SNP için genotip matrisi oluşturulacak.\")\n",
        "print(f\"Girdi VCF dosyası: {vcf_file_for_genotypes}\")\n",
        "\n",
        "# 1. Hızlı arama için seçilen SNP ID'lerinden bir set oluşturalım.\n",
        "# final_selected_snps içindeki her bir tuple'ın 2. indeksi SNP ID'siydi.\n",
        "selected_snp_ids_set = {snp_info[2] for snp_info in final_selected_snps}\n",
        "# İsteğe bağlı: Sütunların belirli bir sırada (örn: p-değerine göre veya pozisyona göre) olması için\n",
        "# SNP ID'lerini final_selected_snps listesindeki sırayla tutabiliriz.\n",
        "# Bu liste, DataFrame'deki sütun sırasını belirlemek için kullanılacak.\n",
        "# final_selected_snps zaten p-değerine göre sıralıydı.\n",
        "ordered_snp_ids_for_columns = [snp_info[2] for snp_info in final_selected_snps]\n",
        "\n",
        "# 2. VCF dosyasını aç ve örnek ID'lerini al\n",
        "try:\n",
        "    vcf_reader_gt = VCF(vcf_file_for_genotypes)\n",
        "except OSError as e:\n",
        "    print(f\"HATA: VCF dosyası ({vcf_file_for_genotypes}) veya indeksi bulunamadı/okunamadı: {e}\")\n",
        "    raise\n",
        "\n",
        "sample_ids = vcf_reader_gt.samples\n",
        "print(f\"{len(sample_ids)} örnek (birey) bulundu.\")\n",
        "\n",
        "# 3. Genotip verilerini toplamak için bir dictionary başlat\n",
        "# İlk sütun olarak örnek ID'lerini ekleyelim\n",
        "genotype_data_for_df = {'SampleID': sample_ids}\n",
        "\n",
        "# 4. VCF dosyasını oku ve seçilen SNP'ler için genotipleri çıkar\n",
        "processed_snps_count = 0\n",
        "found_selected_snps_in_vcf = 0\n",
        "\n",
        "print(\"VCF dosyası okunuyor ve genotipler çıkarılıyor...\")\n",
        "for variant in vcf_reader_gt:\n",
        "    variant_id = variant.ID # bcftools annotate ile oluşturduğumuz ID (örn: CHR:POS:REF:ALT)\n",
        "\n",
        "    if variant_id in selected_snp_ids_set:\n",
        "        found_selected_snps_in_vcf += 1\n",
        "\n",
        "        # Alternatif alel sayılarını al (0, 1, 2). Eksikse np.nan\n",
        "        alt_allele_counts = []\n",
        "        for gt_array in variant.genotypes:\n",
        "            # gt_array = [allele_idx1, allele_idx2, is_phased]\n",
        "            # allele_idx: 0=REF, 1=ALT1, -1=eksik\n",
        "            allele1 = gt_array[0]\n",
        "            allele2 = gt_array[1]\n",
        "\n",
        "            if allele1 == -1 or allele2 == -1: # Eğer genotipin bir kısmı bile eksikse, tüm genotipi eksik sayalım\n",
        "                alt_allele_counts.append(np.nan)\n",
        "            else:\n",
        "                # Biallelik varsayımıyla (REF=0, ALT=1), toplamları alternatif alel sayısını verir\n",
        "                alt_allele_counts.append(allele1 + allele2)\n",
        "\n",
        "        genotype_data_for_df[variant_id] = alt_allele_counts\n",
        "\n",
        "        if found_selected_snps_in_vcf % 500 == 0: # Her 500 SNP'de bir ilerleme bildir\n",
        "            print(f\"  {found_selected_snps_in_vcf}/{len(selected_snp_ids_set)} seçilmiş SNP işlendi...\")\n",
        "\n",
        "vcf_reader_gt.close()\n",
        "print(f\"VCF okuma tamamlandı. {found_selected_snps_in_vcf} adet seçilmiş SNP için genotip verisi toplandı.\")\n",
        "\n",
        "if found_selected_snps_in_vcf != len(selected_snp_ids_set):\n",
        "    print(f\"UYARI: ANOVA ile seçilen {len(selected_snp_ids_set)} SNP'den sadece {found_selected_snps_in_vcf} tanesi VCF dosyasında bulundu!\")\n",
        "    print(\"Bu durum, SNP ID eşleşmesinde bir sorun olduğuna veya VCF dosyasının beklenenden farklı olduğuna işaret edebilir.\")\n",
        "    # Eksik SNP'leri bulmak için (isteğe bağlı):\n",
        "    # found_ids_in_vcf_set = set(genotype_data_for_df.keys())\n",
        "    # missing_from_vcf = selected_snp_ids_set - found_ids_in_vcf_set\n",
        "    # print(f\"VCF'de bulunamayan SNP ID'lerinden bazıları: {list(missing_from_vcf)[:10]}\")\n",
        "\n",
        "\n",
        "# 5. Pandas DataFrame oluştur\n",
        "# Sütunların doğru sırada olması için 'ordered_snp_ids_for_columns' listesini kullanalım.\n",
        "# Ancak, VCF'de bulunamayan SNP'ler varsa, bu sütunlar DataFrame'de olmayacaktır.\n",
        "# Bu yüzden, DataFrame'i oluşturduktan sonra mevcut sütunları kontrol edip sıralamak daha güvenli olabilir\n",
        "# veya sadece genotype_data_for_df'teki sırayı kullanabiliriz (Python 3.7+ dict'ler sıralı).\n",
        "# Şimdilik, 'ordered_snp_ids_for_columns' listesindeki mevcut olanları alalım:\n",
        "final_columns_for_df = ['SampleID'] + [snp_id for snp_id in ordered_snp_ids_for_columns if snp_id in genotype_data_for_df]\n",
        "\n",
        "try:\n",
        "    genotype_df = pd.DataFrame(genotype_data_for_df)\n",
        "    # Sütunları 'SampleID' ve ardından p-değerine göre sıralanmış SNP'ler olacak şekilde ayarla\n",
        "    genotype_df = genotype_df[final_columns_for_df]\n",
        "except KeyError as e:\n",
        "    print(f\"DataFrame oluşturulurken sütun hatası: {e}\")\n",
        "    print(\"Muhtemelen 'ordered_snp_ids_for_columns' ile 'genotype_data_for_df' arasında bir uyumsuzluk var.\")\n",
        "    print(\"DataFrame mevcut sütunlarla oluşturuluyor...\")\n",
        "    genotype_df = pd.DataFrame(genotype_data_for_df) # Ham haliyle oluştur\n",
        "\n",
        "\n",
        "print(f\"\\nDataFrame oluşturuldu. Boyut: {genotype_df.shape[0]} satır, {genotype_df.shape[1]} sütun.\")\n",
        "print(\"İlk 5 satır ve ilk birkaç sütun:\")\n",
        "print(genotype_df.iloc[:5, :min(6, genotype_df.shape[1])]) # İlk 5 satır, en fazla ilk 6 sütun\n",
        "\n",
        "# 6. DataFrame'i csv dosyasına kaydet\n",
        "print(f\"\\nDataFrame '{output_csv_file}' dosyasına kaydediliyor...\")\n",
        "try:\n",
        "    genotype_df.to_csv(output_csv_file, index=False)\n",
        "    print(f\"'{output_csv_file}' başarıyla kaydedildi!\")\n",
        "    # Colab'den indirmek için link (isteğe bağlı)\n",
        "    # from google.colab import files\n",
        "    # files.download(output_csv_file)\n",
        "except Exception as e:\n",
        "    print(f\"csv dosyasına kaydetme sırasında bir hata oluştu: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_E85X-gRhTiN",
        "outputId": "a43465c0-bb09-4342-9c51-db24dac626b2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seçilen 5000 SNP için genotip matrisi oluşturulacak.\n",
            "Girdi VCF dosyası: ALL.chr22.filtered_maf005_ids_ldpruned.vcf.gz\n",
            "2504 örnek (birey) bulundu.\n",
            "VCF dosyası okunuyor ve genotipler çıkarılıyor...\n",
            "  500/5000 seçilmiş SNP işlendi...\n",
            "  1000/5000 seçilmiş SNP işlendi...\n",
            "  1500/5000 seçilmiş SNP işlendi...\n",
            "  2000/5000 seçilmiş SNP işlendi...\n",
            "  2500/5000 seçilmiş SNP işlendi...\n",
            "  3000/5000 seçilmiş SNP işlendi...\n",
            "  3500/5000 seçilmiş SNP işlendi...\n",
            "  4000/5000 seçilmiş SNP işlendi...\n",
            "  4500/5000 seçilmiş SNP işlendi...\n",
            "  5000/5000 seçilmiş SNP işlendi...\n",
            "VCF okuma tamamlandı. 5000 adet seçilmiş SNP için genotip verisi toplandı.\n",
            "\n",
            "DataFrame oluşturuldu. Boyut: 2504 satır, 5001 sütun.\n",
            "İlk 5 satır ve ilk birkaç sütun:\n",
            "  SampleID  '22:17858962:T:C'  '22:18540577:T:C'  '22:20499061:T:A'  \\\n",
            "0  HG00096                  0                  1                  0   \n",
            "1  HG00097                  0                  2                  0   \n",
            "2  HG00099                  0                  0                  0   \n",
            "3  HG00100                  0                  1                  0   \n",
            "4  HG00101                  0                  0                  0   \n",
            "\n",
            "   '22:20950079:T:A'  '22:22568112:T:C'  \n",
            "0                  0                  0  \n",
            "1                  0                  0  \n",
            "2                  0                  0  \n",
            "3                  0                  0  \n",
            "4                  0                  0  \n",
            "\n",
            "DataFrame 'genotip_matrisi_top5000_snps.csv' dosyasına kaydediliyor...\n",
            "'genotip_matrisi_top5000_snps.csv' başarıyla kaydedildi!\n"
          ]
        }
      ]
    }
  ]
}